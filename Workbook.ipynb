{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Sign Recognition Project\n",
    "The goals / steps of this project are the following:\n",
    "* Load the data set\n",
    "* Explore, summarize and visualize the data set\n",
    "* Design, train and test a model architecture\n",
    "* Use the model to make predictions on new images\n",
    "* Analyze the softmax probabilities of the new images\n",
    "* Summarize the results with a written report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the and , or , not and xor function\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the weights\n",
    "theta0 = -30\n",
    "theta1 = 0\n",
    "theta2 = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [(0,0),(0,1),(1,0),(1,1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = theta1 * test_input[0] + theta2 * test_input[1] + theta0\n",
    "    result = int(linear_combination >= 0)\n",
    "    output.append(result)\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8807970779778823\n"
     ]
    }
   ],
   "source": [
    "# gradient descent for logistic regression\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5])\n",
    "\n",
    "# Calculate one gradient descent step for each weight\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = sigmoid(np.dot(x, w))\n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = y-nn_output\n",
    "error_term = error * sigmoid_prime(np.dot(x,w))\n",
    "# TODO: Calculate change in weights\n",
    "del_w = learnrate * error_term * x\n",
    "\n",
    "print(sigmoid(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propogation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in =(np.dot(X,weights_input_to_hidden))\n",
    "hidden_layer_out =sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out,weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor flow implementation of Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "print('All modules imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading notMNIST_train.zip...\n",
      "Download Finished\n",
      "Downloading notMNIST_test.zip...\n",
      "Download Finished\n",
      "All files downloaded.\n"
     ]
    }
   ],
   "source": [
    "def download(url, file):\n",
    "    \"\"\"\n",
    "    Download file from <url>\n",
    "    :param url: URL to file\n",
    "    :param file: Local file path\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file):\n",
    "        print('Downloading ' + file + '...')\n",
    "        urlretrieve(url, file)\n",
    "        print('Download Finished')\n",
    "\n",
    "# Download the training and test dataset.\n",
    "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_train.zip', 'notMNIST_train.zip')\n",
    "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_test.zip', 'notMNIST_test.zip')\n",
    "\n",
    "# Make sure the files aren't corrupted\n",
    "assert hashlib.md5(open('notMNIST_train.zip', 'rb').read()).hexdigest() == 'c8673b3f28f489e9cdf3a3d74e2ac8fa',\\\n",
    "        'notMNIST_train.zip file is corrupted.  Remove the file and try again.'\n",
    "assert hashlib.md5(open('notMNIST_test.zip', 'rb').read()).hexdigest() == '5d3c7e653e63471c88df796156a9dfa9',\\\n",
    "        'notMNIST_test.zip file is corrupted.  Remove the file and try again.'\n",
    "\n",
    "# Wait until you see that all files have been downloaded.\n",
    "print('All files downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 210001/210001 [01:02<00:00, 3378.62files/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 10001/10001 [00:05<00:00, 1915.55files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features and labels uncompressed.\n"
     ]
    }
   ],
   "source": [
    "def uncompress_features_labels(file):\n",
    "    \"\"\"\n",
    "    Uncompress features and labels from a zip file\n",
    "    :param file: The zip file to extract the data from\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with ZipFile(file) as zipf:\n",
    "        # Progress Bar\n",
    "        filenames_pbar = tqdm(zipf.namelist(), unit='files')\n",
    "        \n",
    "        # Get features and labels from all files\n",
    "        for filename in filenames_pbar:\n",
    "            # Check if the file is a directory\n",
    "            if not filename.endswith('/'):\n",
    "                with zipf.open(filename) as image_file:\n",
    "                    image = Image.open(image_file)\n",
    "                    image.load()\n",
    "                    # Load image data as 1 dimensional array\n",
    "                    # We're using float32 to save on memory space\n",
    "                    feature = np.array(image, dtype=np.float32).flatten()\n",
    "\n",
    "                # Get the the letter from the filename.  This is the letter of the image.\n",
    "                label = os.path.split(filename)[1][0]\n",
    "\n",
    "                features.append(feature)\n",
    "                labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Get the features and labels from the zip files\n",
    "train_features, train_labels = uncompress_features_labels('notMNIST_train.zip')\n",
    "test_features, test_labels = uncompress_features_labels('notMNIST_test.zip')\n",
    "\n",
    "# Limit the amount of data to work with a docker container\n",
    "docker_size_limit = 150000\n",
    "train_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)\n",
    "\n",
    "# Set flags for feature engineering.  This will prevent you from skipping an important step.\n",
    "is_features_normal = False\n",
    "is_labels_encod = False\n",
    "\n",
    "# Wait until you see that all features and labels have been uncompressed.\n",
    "print('All features and labels uncompressed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed!\n"
     ]
    }
   ],
   "source": [
    "# Problem 1 - Implement Min-Max scaling for grayscale image data\n",
    "def normalize_grayscale(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Min-Max scaling for grayscale image data\n",
    "    return (0.1+((image_data*(0.9-0.1))/(255)))\n",
    "\n",
    "\n",
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "# Test Cases\n",
    "np.testing.assert_array_almost_equal(\n",
    "    normalize_grayscale(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 255])),\n",
    "    [0.1, 0.103137254902, 0.106274509804, 0.109411764706, 0.112549019608, 0.11568627451, 0.118823529412, 0.121960784314,\n",
    "     0.125098039216, 0.128235294118, 0.13137254902, 0.9],\n",
    "    decimal=3)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    normalize_grayscale(np.array([0, 1, 10, 20, 30, 40, 233, 244, 254,255])),\n",
    "    [0.1, 0.103137254902, 0.13137254902, 0.162745098039, 0.194117647059, 0.225490196078, 0.830980392157, 0.865490196078,\n",
    "     0.896862745098, 0.9])\n",
    "\n",
    "if not is_features_normal:\n",
    "    train_features = normalize_grayscale(train_features)\n",
    "    test_features = normalize_grayscale(test_features)\n",
    "    is_features_normal = True\n",
    "\n",
    "print('Tests Passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels One-Hot Encoded\n"
     ]
    }
   ],
   "source": [
    "if not is_labels_encod:\n",
    "    # Turn labels into numbers and apply One-Hot Encoding\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(train_labels)\n",
    "    train_labels = encoder.transform(train_labels)\n",
    "    test_labels = encoder.transform(test_labels)\n",
    "\n",
    "    # Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
    "    train_labels = train_labels.astype(np.float32)\n",
    "    test_labels = test_labels.astype(np.float32)\n",
    "    is_labels_encod = True\n",
    "\n",
    "print('Labels One-Hot Encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features and labels randomized and split.\n"
     ]
    }
   ],
   "source": [
    "assert is_features_normal, 'You skipped the step to normalize the features'\n",
    "assert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'\n",
    "\n",
    "# Get randomized datasets for training and validation\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    test_size=0.05,\n",
    "    random_state=832289)\n",
    "\n",
    "print('Training features and labels randomized and split.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to pickle file...\n",
      "Data cached in pickle file.\n"
     ]
    }
   ],
   "source": [
    "# Save the data for easy access\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "if not os.path.isfile(pickle_file):\n",
    "    print('Saving data to pickle file...')\n",
    "    try:\n",
    "        with open('notMNIST.pickle', 'wb') as pfile:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    'train_dataset': train_features,\n",
    "                    'train_labels': train_labels,\n",
    "                    'valid_dataset': valid_features,\n",
    "                    'valid_labels': valid_labels,\n",
    "                    'test_dataset': test_features,\n",
    "                    'test_labels': test_labels,\n",
    "                },\n",
    "                pfile, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "\n",
    "print('Data cached in pickle file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and modules loaded.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Load the modules\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reload the data\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  pickle_data = pickle.load(f)\n",
    "  train_features = pickle_data['train_dataset']\n",
    "  train_labels = pickle_data['train_labels']\n",
    "  valid_features = pickle_data['valid_dataset']\n",
    "  valid_labels = pickle_data['valid_labels']\n",
    "  test_features = pickle_data['test_dataset']\n",
    "  test_labels = pickle_data['test_labels']\n",
    "  del pickle_data  # Free up memory\n",
    "\n",
    "\n",
    "print('Data and modules loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_count = 784\n",
    "labels_count = 10\n",
    "\n",
    "# TODO: Set the features and labels tensors\n",
    "features = tf.placeholder(tf.float32,[None,features_count])\n",
    "labels = tf.placeholder(tf.float32,[None,labels_count])\n",
    "\n",
    "# TODO: Set the weights and biases tensors\n",
    "weights = tf.Variable(tf.truncated_normal([features_count,labels_count]))\n",
    "biases = tf.Variable(tf.zeros([labels_count]))\n",
    "\n",
    "# Feed dicts for training, validation, and test session\n",
    "train_feed_dict = {features: train_features, labels: train_labels}\n",
    "valid_feed_dict = {features: valid_features, labels: valid_labels}\n",
    "test_feed_dict = {features: test_features, labels: test_labels}\n",
    "\n",
    "# Linear Function WX + b\n",
    "logits = tf.matmul(features, weights) + biases\n",
    "\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy\n",
    "cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), axis=1)\n",
    "\n",
    "# Training loss\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Create an operation that initializes all variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy function created.\n"
     ]
    }
   ],
   "source": [
    "# Determine if the predictions are correct\n",
    "is_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "# Calculate the accuracy of the predictions\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
    "\n",
    "print('Accuracy function created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1/4: 100%|█████████████████████████████████████████████████████████████| 2850/2850 [01:04<00:00, 44.01batches/s]\n",
      "Epoch  2/4: 100%|█████████████████████████████████████████████████████████████| 2850/2850 [01:07<00:00, 42.05batches/s]\n",
      "Epoch  3/4: 100%|█████████████████████████████████████████████████████████████| 2850/2850 [01:11<00:00, 39.97batches/s]\n",
      "Epoch  4/4: 100%|█████████████████████████████████████████████████████████████| 2850/2850 [01:20<00:00, 35.59batches/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy at 0.09573332965373993\n"
     ]
    }
   ],
   "source": [
    "# TODO: Find the best parameters for each configuration\n",
    "epochs = 4\n",
    "batch_size = 50\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "\n",
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)    \n",
    "\n",
    "# The accuracy measured against the validation set\n",
    "validation_accuracy = 0.0\n",
    "\n",
    "# Measurements use for graphing loss and accuracy\n",
    "log_batch_step = 50\n",
    "batches = []\n",
    "loss_batch = []\n",
    "train_acc_batch = []\n",
    "valid_acc_batch = []\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    batch_count = int(math.ceil(len(train_features)/batch_size))\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        # Progress bar\n",
    "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
    "        \n",
    "        # The training cycle\n",
    "        for batch_i in batches_pbar:\n",
    "            # Get a batch of training features and labels\n",
    "            batch_start = batch_i*batch_size\n",
    "            batch_features = train_features[batch_start:batch_start + batch_size]\n",
    "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "\n",
    "            # Run optimizer and get loss\n",
    "            _, l = session.run(\n",
    "                [optimizer, loss],\n",
    "                feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "            # Log every 50 batches\n",
    "            if not batch_i % log_batch_step:\n",
    "                # Calculate Training and Validation accuracy\n",
    "                training_accuracy = session.run(accuracy, feed_dict=train_feed_dict)\n",
    "                validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)\n",
    "\n",
    "                # Log batches\n",
    "                previous_batch = batches[-1] if batches else 0\n",
    "                batches.append(log_batch_step + previous_batch)\n",
    "                loss_batch.append(l)\n",
    "                train_acc_batch.append(training_accuracy)\n",
    "                valid_acc_batch.append(validation_accuracy)\n",
    "\n",
    "        # Check accuracy against Validation data\n",
    "        validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)\n",
    "\n",
    "loss_plot = plt.subplot(211)\n",
    "loss_plot.set_title('Loss')\n",
    "loss_plot.plot(batches, loss_batch, 'g')\n",
    "loss_plot.set_xlim([batches[0], batches[-1]])\n",
    "acc_plot = plt.subplot(212)\n",
    "acc_plot.set_title('Accuracy')\n",
    "acc_plot.plot(batches, train_acc_batch, 'r', label='Training Accuracy')\n",
    "acc_plot.plot(batches, valid_acc_batch, 'x', label='Validation Accuracy')\n",
    "acc_plot.set_ylim([0, 1.0])\n",
    "acc_plot.set_xlim([batches[0], batches[-1]])\n",
    "acc_plot.legend(loc=4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Validation accuracy at {}'.format(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1/4: 100%|███████████████████████████████████████████████████████████| 2850/2850 [00:02<00:00, 1083.50batches/s]\n",
      "Epoch  2/4: 100%|████████████████████████████████████████████████████████████| 2850/2850 [00:03<00:00, 806.91batches/s]\n",
      "Epoch  3/4: 100%|████████████████████████████████████████████████████████████| 2850/2850 [00:05<00:00, 533.01batches/s]\n",
      "Epoch  4/4: 100%|████████████████████████████████████████████████████████████| 2850/2850 [00:05<00:00, 482.65batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice Job! Test Accuracy is 0.8607000112533569\n"
     ]
    }
   ],
   "source": [
    "# TODO: Set the epochs, batch_size, and learning_rate with the best parameters from problem 3\n",
    "epochs = 4\n",
    "batch_size = 50\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "\n",
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "# The accuracy measured against the test set\n",
    "test_accuracy = 0.0\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    session.run(init)\n",
    "    batch_count = int(math.ceil(len(train_features)/batch_size))\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        # Progress bar\n",
    "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
    "        \n",
    "        # The training cycle\n",
    "        for batch_i in batches_pbar:\n",
    "            # Get a batch of training features and labels\n",
    "            batch_start = batch_i*batch_size\n",
    "            batch_features = train_features[batch_start:batch_start + batch_size]\n",
    "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "\n",
    "            # Run optimizer\n",
    "            _ = session.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "        # Check accuracy against Test data\n",
    "        test_accuracy = session.run(accuracy, feed_dict=test_feed_dict)\n",
    "\n",
    "\n",
    "assert test_accuracy >= 0.80, 'Test accuracy at {}, should be equal to or greater than 0.80'.format(test_accuracy)\n",
    "print('Nice Job! Test Accuracy is {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELU hidden layer\n",
    "\n",
    "hidden_layer = tf.add(tf.matmul(features,hidden_weights),hidden_biases)\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer,output_weights),output_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout logic\n",
    "\n",
    "keep_prob=tf.placeholder(float32) # probability to keep units\n",
    "\n",
    "#once hidden layer is calculated , perform dropout\n",
    "hidden_layer = tf.nn.dropout(hidden_layer,keep_prob)\n",
    "# keep_prob is normally 0.5 for the training set and 1.0 for the test and validation set\n",
    "# after dropout is done, calculate the output or logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor flow convolution layer\n",
    "k_output = 64\n",
    "\n",
    "#image properties\n",
    "\n",
    "image_height = 10\n",
    "image_width = 10\n",
    "color_channels = 3\n",
    "\n",
    "# Convolution filter\n",
    "\n",
    "filter_size_width = 5\n",
    "filter_size_height = 5\n",
    "\n",
    "input_image = tf.placeholder(tf.float32,shape=[None,image_height,image_width,color_channels])\n",
    "\n",
    "# Weights and biases\n",
    "weight = tf.Variable(tf.truncated_normal([filter_size_height, filter_size_width, color_channels, k_output]))\n",
    "bias = tf.Variable(tf.zeros(k_output))\n",
    "\n",
    "conv_layer = tf.nn.conv2d(input_image,weight,strides=[1,2,2,1],padding='SAME')\n",
    "conv_layer = tf.nn.bias_add(conv_layer,bias)\n",
    "\n",
    "# Apply activation function\n",
    "conv_layer = tf.nn.relu(conv_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## applying a max pooling layer to the conv layer\n",
    "\n",
    "conv_layer = tf.nn.max_pool(conv_layer,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "# k-size is filter size and strides is the stride step of the max pool layer represented\n",
    "# in [batch,height,width,channels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating a series of Convolutional networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.05\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "# number of samples to calculate validation accuracy\n",
    "test_valid_size = 128\n",
    "\n",
    "# Network parameters\n",
    "n_classes = 10\n",
    "dropout = 0.75\n",
    "\n",
    "# Weights and biases\n",
    "# out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "# out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "\n",
    "weights = { 'wc1' : tf.Variable(tf.random_normal([5,5,1,32])),\n",
    "            'wc2' : tf.Variable(tf.random_normal([5,5,32,64])),\n",
    "            'wd1' : tf.Variable(tf.random_normal([7*7*64,1024])),\n",
    "            'out' : tf.Variable(tf.random_normal([1024,n_classes]))\n",
    "          }\n",
    "biases = {  'wc1' : tf.Variable(tf.random_normal([32])),\n",
    "            'wc2' : tf.Variable(tf.random_normal([64])),\n",
    "            'wd1' : tf.Variable(tf.random_normal([1024])),\n",
    "            'out' : tf.Variable(tf.random_normal([n_classes]))\n",
    "         }\n",
    "\n",
    "def conv2d(x,W,b,strides=1):\n",
    "    x=tf.nn.conv2d(x,W,strides=[1,strides,strides,1],padding='SAME')\n",
    "    x=tf.nn.bias_add(x,b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def max_pool(x,k=2):\n",
    "    x=tf.nn.max_pool(x,k_size=[1,k,k,1],strides=[1,k,k,1],padding='SAME')\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the code below, we're creating 3 layers alternating between convolutions and max pooling \n",
    "# followed by a fully connected and output layer. \n",
    "\n",
    "def conv_net(x,weights,biases,dropout):\n",
    "    \n",
    "    # Layer 1 - 28*28*1 image - 14*14*32\n",
    "    conv1 = conv2d(x,weights['wc1'],biases['wc1'])\n",
    "    conv1 = max_pool(conv1)\n",
    "    \n",
    "    # convolution layer 2 - 14*14*32 to 7*7*64\n",
    "    conv2 = conv2d(conv1,weights['wc2'],biases['wc2'])\n",
    "    conv2 = conv2d(conv2)\n",
    "    \n",
    "    # fully connected layer\n",
    "    fc1 = tf.reshape(conv2,[-1,weights['wd1'].getshape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1,weights['wd1']),biases['wd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.droput(fc1,dropout)\n",
    "    \n",
    "    out = tf.add(tf.matmul(fc1,weights['out']),biases['out'])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None,28,28,1])\n",
    "y = tf.placeholder(tf.float32,[None,n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# create model\n",
    "logits = convnet(x,weights,biases,keep_prob)\n",
    "\n",
    "# define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y))\n",
    "optimizer = tf.nn.Gradient_Descent_optimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1),tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variable_initilizer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(mnist.train.num_examples//batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={x:batch_x,y:batch_y,keep_prob:dropout})\n",
    "            \n",
    "            loss = sess.run(cost,feed_dict = {x: batch_x,y: batch_y,keep_prob: 1})\n",
    "            valid_acc = sess.run(accuracy, feed_dict={x: mnist.validation.images[:test_valid_size], \n",
    "                                                      y: mnist.validation.labels[:test_valid_size],\n",
    "                                                      keep_prob: 1.})\n",
    "\n",
    "            print('Epoch {:>2}, Batch {:>3} -'\n",
    "                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(epoch + 1,batch + 1,loss,valid_acc))\n",
    "            \n",
    "    # calculate test accuracy\n",
    "    test_acc = sess.run(accuracy,feed_dict{x: mnist.validation.images[:test_valid_size],\n",
    "                                           y: mnist.validation.labels[:test_valid_size],\n",
    "                                           keep_prob: 1.})\n",
    "    \n",
    "    print('Testing Accuracy: {}'.format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
